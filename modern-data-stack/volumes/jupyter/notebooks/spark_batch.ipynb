{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4c8116-8744-4106-acf2-a4c6cdad69cb",
   "metadata": {},
   "source": [
    "# Laboratório: Aprender a ler dados RAW com PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625895e3-1f80-4785-b569-4832f33f8e54",
   "metadata": {},
   "source": [
    "## 1. Instale as bibliotecas necessárias (se ainda não instalou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b30f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minio pandas pyarrow pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4beee-7c15-4599-95c5-8059f191df93",
   "metadata": {},
   "source": [
    "## 2. Importe as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8154ab7-476b-4e22-bebc-d8919701aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80415395-fa70-443a-839d-49973e257ffd",
   "metadata": {},
   "source": [
    "## 3. Configure as credenciais e endpoint do MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7217e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "    \"minio:9000\",\n",
    "    access_key=\"4PRJYFLGzQYTnOJGH1gA\",\n",
    "    secret_key=\"ovBkCsqh2cXNkyoteCzQMV5JWCUk5tHfsG1GwYbD\",\n",
    "    secure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c3545-b45d-40a0-8646-d3c0659b24fa",
   "metadata": {},
   "source": [
    "## 4. Liste os arquivos Parquet do bucket 'raw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista todos os arquivos Parquet disponíveis no bucket 'raw' do MinIO.\n",
    "bucket_name = \"raw\"\n",
    "objects = minio_client.list_objects(bucket_name, recursive=True)\n",
    "parquet_files = [obj.object_name for obj in objects if obj.object_name.endswith('.parquet')]\n",
    "print(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cada arquivo Parquet (tabela), leia os dados em um DataFrame e armazene em um dicionário.\n",
    "tables = {}\n",
    "for file in parquet_files:\n",
    "    response = minio_client.get_object(bucket_name, file)\n",
    "    data = response.read()\n",
    "    df = pd.read_parquet(io.BytesIO(data))\n",
    "    #df.drop_duplicates(inplace=True)  # Remove duplicatas, se necessário\n",
    "    #df.dropna(inplace=True)  # Remove linhas com valores ausentes, se necessário\n",
    "    table_name = file.split('/')[-2] # Extrai o nome da tabela do caminho do arquivo\n",
    "    tables[table_name] = df\n",
    "    response.close()\n",
    "    response.release_conn()\n",
    "\n",
    "# Agora, 'tables' é um dicionário onde a chave é o nome da tabela e o valor é o DataFrame correspondente.\n",
    "tables['film']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3fe01",
   "metadata": {},
   "source": [
    "## 5. Criando um Warehouse no LakeKeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9709f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Definindo a URL base da API do Lakekeeper\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "MANAGEMENT_URL = \"http://lakekeeper:8181/management\"\n",
    "WAREHOUSE = \"trusted\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{MANAGEMENT_URL}/v1/warehouse\",\n",
    "    json={\n",
    "        \"warehouse-name\": WAREHOUSE,\n",
    "        \"storage-profile\": {\n",
    "            \"type\": \"s3\",\n",
    "            \"bucket\": \"trusted\",\n",
    "            \"flavor\": \"minio\",\n",
    "            \"key-prefix\": None,\n",
    "            \"assume-role-arn\": None,\n",
    "            \"endpoint\": \"http://minio:9000\",\n",
    "            \"region\": \"local-01\",\n",
    "            \"path-style-access\": True,\n",
    "            \"sts-enabled\": False\n",
    "        },\n",
    "        \"storage-credential\": {\n",
    "            \"type\": \"s3\",\n",
    "            \"credential-type\": \"access-key\",\n",
    "            \"aws-access-key-id\": \"4PRJYFLGzQYTnOJGH1gA\",\n",
    "            \"aws-secret-access-key\": \"ovBkCsqh2cXNkyoteCzQMV5JWCUk5tHfsG1GwYbD\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"{response.status_code}: {response.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b262b4-5d0d-4b70-bb79-08f971da580f",
   "metadata": {},
   "source": [
    "## 6. Criando uma tabela com PySpark e Lakekeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do Spark para integração com o catálogo Iceberg via REST, utilizando o Lakekeeper e MinIO.\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.7.0\"\n",
    "CATALOG = \"trusted\"\n",
    "\n",
    "config = {\n",
    "    \"spark.sql.defaultCatalog\": \"trusted\",\n",
    "    f\"spark.sql.catalog.{CATALOG}\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.{CATALOG}.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.{CATALOG}.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.{CATALOG}.warehouse\": WAREHOUSE,\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.jars.packages\": f\"\"\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\"\"\"\n",
    "}\n",
    "\n",
    "print(config)\n",
    "spark_config = SparkConf().setMaster('local').setAppName(\"Iceberg-REST\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "spark.sql(f\"USE {CATALOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80fbbb-354c-4ef0-a20b-a85169c07f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do namespace (banco de dados) 'pagila_db' no catálogo Iceberg, caso não exista.\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS pagila_db\")\n",
    "spark.sql(\"SHOW NAMESPACES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19da8b",
   "metadata": {},
   "source": [
    "## 7. Agrupando as partições de pagamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ce474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista chamada payment_dfs para armazenar os DataFrames relacionados a pagamentos\n",
    "payment_dfs = []\n",
    "\n",
    "# Percorre todas as tabelas carregadas\n",
    "for nome_tabela, df in tables.items():\n",
    "    # Se o nome da tabela contém a palavra 'payment', adiciona o DataFrame à lista\n",
    "    if 'payment' in nome_tabela:\n",
    "        payment_dfs.append(df)\n",
    "\n",
    "# Cria um DataFrame Spark a partir do DataFrame concatenado e escreve na tabela 'pagila_db.payments'\n",
    "if payment_dfs:\n",
    "    payments_df = pd.concat(payment_dfs, ignore_index=True)\n",
    "    sdf = spark.createDataFrame(payments_df)\n",
    "    sdf.writeTo(\"pagila_db.payments\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f299c2-ecda-44a1-aaf0-9897e209d5e3",
   "metadata": {},
   "source": [
    "## 8. Criando as demais tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fa6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve as demais tabelas no catálogo Iceberg, exceto as já processadas de pagamento.\n",
    "for table_name, df in tables.items():\n",
    "    if 'payment' in table_name:\n",
    "        continue  # já processado acima\n",
    "    if not df.empty:\n",
    "        sdf = spark.createDataFrame(df)\n",
    "        sdf.writeTo(f\"pagila_db.{table_name}\").createOrReplace()\n",
    "    else:\n",
    "        print(f\"Skipping empty DataFrame for table: {table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
