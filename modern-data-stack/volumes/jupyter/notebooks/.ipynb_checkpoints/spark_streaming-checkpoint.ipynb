{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2b7221-bad0-459d-9a4e-48d483b692cc",
   "metadata": {},
   "source": [
    "## 1. Instale as bibliotecas necessárias (se ainda não instalou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fb93f5-a607-4600-8374-67707f41c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (17.0.0)\n",
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c1eaa-4a21-4971-95bd-70e7c6e9ada3",
   "metadata": {},
   "source": [
    "## 2. Importe as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c7d5f5-58a3-4d86-a691-2690bc4feb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-aws-bundle:1.7.0,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a520389f-699f-451b-a5e7-f01272d286d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, current_timestamp, to_timestamp,\n",
    "    year, month, dayofmonth, hour\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, TimestampType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffa18f-d88e-410a-83fd-d2e1240b4ec1",
   "metadata": {},
   "source": [
    "# Variáveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cdcdaa-3fb8-40e1-b404-2ad37598b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "CATALOG = \"trusted\"\n",
    "WAREHOUSE = \"trusted\"\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = \"4PRJYFLGzQYTnOJGH1gA\"\n",
    "MINIO_SECRET_KEY = \"ovBkCsqh2cXNkyoteCzQMV5JWCUk5tHfsG1GwYbD\"\n",
    "CHECKPOINT_LOCATION = \"s3a://checkpoint/popular_critics\"\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'kafka:9092'\n",
    "KAFKA_TOPIC = \"popular_critics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3457a-3467-4618-93a5-701d561e3094",
   "metadata": {},
   "source": [
    "# Schema dos dados de críticas populares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b964a86-168f-4c8f-8c57-f65ac6d71c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULAR_CRITICS_SCHEMA = StructType([\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"film\", StringType(), False),\n",
    "        StructField(\"rating\", DoubleType(), False),\n",
    "        StructField(\"review\", StringType(), False),\n",
    "        StructField(\"ingestion_timestamp\", TimestampType(), False)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c2a5e-95b2-419b-b04f-fb5b76946bfa",
   "metadata": {},
   "source": [
    "# Configuração do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d929a0be-fca3-42d5-be06-283590e0c0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"spark.sql.defaultCatalog\": \"trusted\",\n",
    "    f\"spark.sql.catalog.{CATALOG}\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.{CATALOG}.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.{CATALOG}.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.{CATALOG}.warehouse\": WAREHOUSE,\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.catalog.iceberg_catalog.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.catalog.iceberg_catalog.s3.endpoint\": MINIO_ENDPOINT,\n",
    "    \"spark.sql.catalog.iceberg_catalog.s3.path-style-access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": MINIO_ENDPOINT,\n",
    "    \"spark.hadoop.fs.s3a.access.key\": MINIO_ACCESS_KEY,\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": MINIO_SECRET_KEY,\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint.region\": \"local-01\",\n",
    "    \"spark.sql.streaming.checkpointLocation\": CHECKPOINT_LOCATION,\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.sql.shuffle.partitions\": \"4\"\n",
    "}\n",
    "\n",
    "spark_config = SparkConf().setMaster('local').setAppName(\"PopularCriticsStreamingIngestion\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "spark.sql(f\"USE {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afe32f-d51c-4740-9a35-7b6b795a613b",
   "metadata": {},
   "source": [
    "# Criar a tabela Iceberg (se ela não existir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3160fb68-8497-4ad9-bea4-6c4a5e51c26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS pagila_db.popular_critics (\n",
    "        name STRING,\n",
    "        film STRING,\n",
    "        rating DOUBLE,\n",
    "        review STRING,\n",
    "        ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fbbba-f4cb-49a5-813b-293a5b4c7aef",
   "metadata": {},
   "source": [
    "# Ler dados do tópico Kafka em modo streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90cc6789-8b93-4667-af4d-9914dee73c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "        .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b022fb2-3e65-4105-9d52-f23ba7d50686",
   "metadata": {},
   "source": [
    "# Adicionando coluna de timestamp aos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ff7a72-af86-44a8-aa95-1327caad1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = kafka_df.select(\n",
    "from_json(col(\"value\").cast(\"string\"), POPULAR_CRITICS_SCHEMA).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "transformed_df = parsed_df.withColumn(\"ingestion_timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982010af-0ef0-4b38-837a-ae0485f92c28",
   "metadata": {},
   "source": [
    "# Escrever os dados na tabela Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e6ea5d3-13f3-48d5-bd8c-9802086a0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = transformed_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .option(\"fanout-enabled\", \"true\") \\\n",
    "        .toTable(\"pagila_db.popular_critics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df5331e-cdad-4d37-a70f-95fc9a0ff961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka batch count: 235\n",
      "+----+--------------------+---------------+---------+------+--------------------+-------------+\n",
      "| key|               value|          topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+---------------+---------+------+--------------------+-------------+\n",
      "|NULL|[7B 22 6E 61 6D 6...|popular_critics|        0|     8|2025-12-14 15:10:...|            0|\n",
      "|NULL|[7B 22 6E 61 6D 6...|popular_critics|        0|     9|2025-12-14 15:10:...|            0|\n",
      "|NULL|[7B 22 6E 61 6D 6...|popular_critics|        0|    10|2025-12-14 15:11:...|            0|\n",
      "|NULL|[7B 22 6E 61 6D 6...|popular_critics|        0|    11|2025-12-14 15:11:...|            0|\n",
      "|NULL|[7B 22 6E 61 6D 6...|popular_critics|        0|    12|2025-12-14 15:11:...|            0|\n",
      "+----+--------------------+---------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------------+------+--------------------+-------------------+\n",
      "|          name|                film|rating|              review|ingestion_timestamp|\n",
      "+--------------+--------------------+------+--------------------+-------------------+\n",
      "|  RAFAEL ABNEY|      CRAFT OUTFIELD|   8.0|Um filme incrível...|               NULL|\n",
      "| SHERRI RHODES|RESURRECTION SILV...|   7.0|Um filme incrível...|               NULL|\n",
      "|NORMA GONZALES|      PLUTO OLEANDER|   6.0|O filme é bom, ma...|               NULL|\n",
      "| ALVIN DELOACH|        DESIRE ALIEN|   5.0|Cumpre o que prom...|               NULL|\n",
      "|MARSHA DOUGLAS| SNATCHERS MONTEZUMA|   5.0|Cumpre o que prom...|               NULL|\n",
      "+--------------+--------------------+------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema do DataFrame lido do Kafka:\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- film: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      " |-- ingestion_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Debug: Verifique se há dados chegando do Kafka e se o schema está correto\n",
    "kafka_batch_df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "print(\"Kafka batch count:\", kafka_batch_df.count())\n",
    "kafka_batch_df.show(5)\n",
    "parsed_batch_df = kafka_batch_df.select(from_json(col(\"value\").cast(\"string\"), POPULAR_CRITICS_SCHEMA).alias(\"data\")).select(\"data.*\")\n",
    "parsed_batch_df.show(5)\n",
    "print(\"Schema do DataFrame lido do Kafka:\")\n",
    "parsed_batch_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03314b0-ccd5-4471-b7cf-223091f6f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicie e aguarde o streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaaea02-2c51-4d15-973e-c9b87db936ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query = query  # query já é um StreamingQuery\n",
    "streaming_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450a985-b121-46fd-8b95-405df096d983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
